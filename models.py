import torch
import torch.nn as nn
import torch.nn.functional as F

from torch.autograd import Variable
from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel, RobertaModel
from transformers.models.xlm_roberta.modeling_xlm_roberta import XLMRobertaModel
from transformers import AutoModel
import math
from torch.nn.parameter import Parameter

from processing.utils import SpatialDropout


# Use the GPU if it's available
use_gpu = torch.cuda.is_available()

# Quick utility function to sample from the Gumbel-distribution: -Log(-Log(Uniform)), eps to avoid numerical errors
def sample_gumbel(input_size, eps=1e-20):
    unif = torch.rand(input_size)
    if use_gpu:
        unif = unif.cuda()
    return -torch.log(-torch.log(unif + eps) + eps)

class Encoder(nn.Module):
    '''
    The encoder takes embedding dimension, M, and K (from MxK coding scheme introduced in paper)
    as parameters. From a word's baseline embedding,
    it outputs Gumbel-softmax or one-hot encoding vectors d_w, reshaped for the decoder.
    In the original paper, the hidden_layer was fixed at M * K/2.
    Input shape: BATCH_SIZE X EMBEDDING_DIM
    Output shape: BATCH_SIZE X M X K X 1, Code (K X 1)
    '''
    def __init__(self, emb_size,M, K, hidden_size= None):
        super(Encoder, self).__init__()
        # For GloVE vectors, emb_size = 200
        self.emb_size = emb_size
        self.K = K
        self.M = M
        # If not otherwise specified, use hidden_size indicated by paper
        if not hidden_size:
            hidden_size = int(M * K / 2)
        # This linear layer maps to latent hidden representation
        self.h_w = nn.Linear(self.emb_size, hidden_size)
        # This layer maps from the hidden layer to BATCH_SIZE X M K
        self.alpha_w = nn.Linear(hidden_size, M * K)

    def forward(self, x, tau=1, eps=1e-20, training=True):
        # We apply hidden layer projection from original embedding
        hw = torch.tanh(self.h_w(x))
        # We apply second projection and softplus activation
        alpha = F.softplus(self.alpha_w(hw))
        # This rearranges alpha to be more intuitively BATCH_SIZE X M X K
        alpha = alpha.view(-1, self.M, self.K)
        # Take the log of all elements
        log_alpha = torch.log(alpha)
        # We apply Gumbel-softmax trick to get code vectors d_w
        d_w = F.softmax((log_alpha + sample_gumbel(log_alpha.size())) / tau, dim=-1)
        # Find argmax of all d_w vectors
        _, ind = d_w.max(dim=-1)
        if not training:
            # Allows us when not training to convert soft vector to a hard, binarized one-hot encoding vector
            d_w = torch.zeros_like(d_w).scatter_(-1, ind.unsqueeze(2), 1.0)
        # d_w is now BATCH x M x K x 1
        d_w = d_w.unsqueeze(-1)
        return d_w, ind

class Decoder(nn.Module):
    '''
    The decoder receives d_w as input from the encoder, and outputs the embedding generated by this code.
    It stores a set of source dictionaries, represented by A, and computes the proper embedding from a summation
    of M matrix-vector products.
    INPUT SHAPE: BATCH_SIZE X M X K X 1
    OUTPUT SHAPE: BATCH_SIZE X EMBEDDING_DIM
    '''
    def __init__(self, M, K, output_size):
        super(Decoder, self).__init__()
        self.output_size = output_size
        self.K = K
        self.M = M
        # Contains source dictionaries for computing embedding given codes
        self.A = Source_Dictionary(M, output_size, K)

    # Following the formula in the paper, performs multiplication and summation over the M matrix-vector products
    def forward(self, d_w):
        output = self.A(d_w)
        output = torch.sum(output, dim=1)
        return output


class Code_Learner(nn.Module):
    '''
    Our final autoencoder model structure used to train our encoded embeddings with an end-to-end network
    INPUT: baseline embeddings BATCH_SIZE X EMB_DIMENSIONS
    OUTPUT: BATCH_SIZE X EMBEDDING_DIM (final encoding representation)
    '''
    def __init__(self,emb_size, M, K, hidden_size = None):
        super(Code_Learner, self).__init__()
        # Initialize encoder and decoder components
        self.encoder = Encoder(emb_size,M, K, hidden_size)
        self.decoder = Decoder(M, K, emb_size)

    # Set up basic, normal encoder-decoder structure
    def forward(self, x, tau=1, eps=1e-20, training=True):
        d_w, _ = self.encoder(x, tau, eps, training)
        comp_emb = self.decoder(d_w)
        return comp_emb


class Source_Dictionary(nn.Module):
    r"""I basically modified the source code for the nn.Linear() class
        Removed bias, and the weights are of dimension M X EMBEDDING_SIZE X K
        INPUT: BATCH_SIZE X M X K X 1
        OUTPUT:BATCH_SIZE X K X 1
    """

    def __init__(self, M, emb_size, K):
        super(Source_Dictionary, self).__init__()
        # The weight of the dictionary is the set of M dictionaries of size EMB X K
        self.weight = Parameter(torch.Tensor(M, emb_size, K))
        self.reset_parameters()

    # Initialize parameters of Source_Dictionary
    def reset_parameters(self):
        stdv = 1. / math.sqrt(self.weight.size(1))
        self.weight.data.uniform_(-stdv, stdv)

    # Operation necessary for proper batch matrix multiplication
    def forward(self, input):
        result = torch.matmul(self.weight, input)
        return result.squeeze(-1)

# LSTM classifier
class NeuralNet(nn.Module):
    def __init__(self, embedding_matrix, LSTM_UNITS):
        super(NeuralNet, self).__init__()
        vocab_size, embed_size = embedding_matrix.size()
        # DENSE_HIDDEN_UNITS = LSTM_UNITS*4
        
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.embedding.weight = nn.Parameter(embedding_matrix.clone().detach())
        self.embedding.weight.requires_grad = False
 
        self.embedding_dropout = SpatialDropout(0.2)
      
        self.lstm1 = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)
        self.lstm2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)

        self.linear1 = nn.Linear(LSTM_UNITS * 4, LSTM_UNITS * 4)
        self.linear2 = nn.Linear(LSTM_UNITS * 4, LSTM_UNITS * 4)
        
        self.linear_out = nn.Linear(LSTM_UNITS * 4, 7)
        
    def forward(self, x):
        h_embedding = self.embedding(x)
        h_embedding = self.embedding_dropout(h_embedding)
        
        h_lstm1, _ = self.lstm1(h_embedding)
        h_lstm2, _ = self.lstm2(h_lstm1)
        
        # global average pooling
        avg_pool = torch.mean(h_lstm2, 1)
        # global max pooling
        max_pool, _ = torch.max(h_lstm2, 1)
        
        h_conc = torch.cat((max_pool, avg_pool), 1)
        h_conc_linear1  = F.relu(self.linear1(h_conc))
        h_conc_linear2  = F.relu(self.linear2(h_conc))
        
        hidden = h_conc + h_conc_linear1 + h_conc_linear2
        
        result = self.linear_out(hidden)

        return result
# enviBERT
class BertBase(RobertaPreTrainedModel):
    def __init__(self, conf, model_name):
        super(BertBase, self).__init__(conf)
        self.config = conf
        self.model_name = model_name
        self.num_labels = conf.num_labels
        self.backbone = XLMRobertaModel.from_pretrained(self.model_name, config=self.config)
        self.lstm_units = 768
        self.num_recurrent_layers = 1
        self.bidirectional = False

        self.lstm = nn.LSTM(input_size=self.config.hidden_size,
                            hidden_size=self.lstm_units,
                            num_layers=self.num_recurrent_layers,
                            bidirectional=self.bidirectional,
                            batch_first=True)
        
        self.dropout = nn.Dropout(0.2)  
        self.out = nn.Linear(self.config.hidden_size*2, self.num_labels)
    
    def forward(self, input_ids, attention_mask, token_type_ids):
        
        # with autocast():
            sequence_output, _, hidden_outputs = self.backbone(
                input_ids=input_ids,
                attention_mask=attention_mask,
                token_type_ids=token_type_ids,
                return_dict = False
            )

            # sequence_output = torch.stack(hidden_outputs[-2:]).mean(0)
            # sequence_output = torch.cat(tuple([hidden_outputs[i] for i in [-1, -2]]), dim=-1)

            if self.bidirectional:
                n = 2
            else: n = 1

            h0 = Variable(torch.zeros(self.num_recurrent_layers * n,       # (L * 2 OR L, B, H)
                                      input_ids.shape[0],
                                      self.lstm_units)).cuda()
            c0 = Variable(torch.zeros(self.num_recurrent_layers * n,        # (L * 2 OR L, B, H)
                                      input_ids.shape[0],
                                      self.lstm_units)).cuda()

            sequence_output, _ = self.lstm(sequence_output, (h0, c0))

            avg_pool = torch.mean(sequence_output, 1)
            max_pool, _ = torch.max(sequence_output, 1)        
            h_conc = torch.cat((max_pool, avg_pool), 1)
            output = self.dropout(h_conc)
            logits = self.out(output)

            return logits

# phoBERT 
class BertCNN(RobertaPreTrainedModel):
    def __init__(self, conf, model_name):
        super(BertCNN, self).__init__(conf)
        self.config = conf
        self.model_name = model_name
        self.num_labels = conf.num_labels
        self.backbone = AutoModel.from_pretrained(self.model_name, config=self.config)

        self.convs = nn.ModuleList([nn.Conv1d(256, 256, kernel_size) for kernel_size in [3,5,7]])
        self.dropout = nn.Dropout(0.1)
        self.out = nn.Linear(self.config.hidden_size, self.num_labels)
    
    def forward(self, input_ids, attention_mask, token_type_ids):
        sequence_output, _, hidden_outputs = self.backbone(
            input_ids = input_ids,
            attention_mask = attention_mask,
            token_type_ids = token_type_ids,
            return_dict = False
        )
        sequence_output = torch.stack([hidden_outputs[-1], hidden_outputs[-2], hidden_outputs[-3]])
        sequence_output = torch.mean(sequence_output, dim=0)

       
        ## CNN
        cnn = [F.relu(conv(sequence_output)) for conv in self.convs]
        max_pooling = []
        for i in cnn:
          max, _ = torch.max(i, 2)
          max_pooling.append(max)
        output = torch.cat(max_pooling, 1)
        
        output = self.dropout(output)
        logits = self.out(output)

        return logits


